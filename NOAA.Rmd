---
title: "NOAA Weather Data Analysis 1950-2001"
author: "SDMitchell"
date: "August 15, 2015"
output: 
  html_document:
    keep_md: true
---


## Synopsis
	Immediately after the title, there should be a synopsis which describes and summarizes your analysis in at most 10 complete sentences.
	
    1. Across the United States, which types of events (as indicated in the EVTYPE variable) are most harmful with respect to population health?

    2. Across the United States, which types of events have the greatest economic consequences?
	

## Data Processing


```{r Libraries,echo=FALSE, warning=FALSE}
library(ggplot2)
library(tools)
library(lubridate)
options(digits=2, scipen=1)
```

These variables are for our target data on the internet and our starting filenames.
  
```{r Variables}
cacheFilename <- "repdata_data_StormData.rds"
rawDataFilename <- "repdata_data_StormData.csv.bz2"
targetURL <- "https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2"
```
  
We'll start the data cleaning process by abbreviating the data set to one that continas only the features of interest. Once we have this set we store it on disk as a serialized R object for future use (and much shorter startup times). We store this data set in the variable "originalData".
  
```{r Data Acquisition}
acquireData <- function() {
	# Try to download the given file if our input file does not already exist
	if(!file.exists(cacheFilename))
	{
		# If the input data file doesn't exist yet, we need to download it
		if(!file.exists(rawDataFilename))
		{
			download.file(url=targetURL, destfile=rawDataFilename, method="auto", mode="wb")
		}
	
		if(file.exists(rawDataFilename))
		{
			# Record the downloaded file's MD5 sum; this file's creation date can serve as the downloaded date
			write(md5sum(rawDataFilename), paste(rawDataFilename, ".MD5", sep=""))
		}
		else
		{
			stop("There was a problem attempting to download the file from the given URL")
		}
		# We either have the raw data file or it was already on disk. So here we are going to create an abbreviate data set
		# with just the features we want. This allows us to get rid of the noisy (and memory-heavy) REMARKS column. Plus
		# the time columns are an error-filled mess that were obviously done by manual entry and are nigh unusable.
		dataRaw <- read.csv(rawDataFilename, header=TRUE, sep=",", stringsAsFactors=FALSE, na.strings="NA")
		dataSelection <- dataRaw[, c("BGN_DATE", "STATE", "EVTYPE", "F", "FATALITIES", "INJURIES", "PROPDMG", "PROPDMGEXP",  "CROPDMG", "CROPDMGEXP", "WFO")]
		saveRDS(dataSelection, cacheFilename)
	}
	else
	{
		dataSelection <- readRDS(cacheFilename)
	}
	
	dataSelection
}
originalData <- acquireData()
```
  
In the interest of starting with a tidy data set, we can compact the property damage and crop damage columns by simply multiplying them by their proper exponent column. There are a lot of problems in the exponents column with many entries not matching the documented accepted values, so some assumptions must be made.
  
Invalid Exponent | Assumed Value
-------- | --------
- | 1000
+ | 1000000
? | 0
h/H | 100
Numeric | equivalent number
empty string | 0

```{r Data Cleaning Phase 1}
# Create a mapping of data set value to exponent that we are going to assume it represents
exponent <- as.data.frame(c(3, 6, 0, 9, 6, 6, 0, 5, 6, 0, 4, 2, 3, 2, 7, 2, 3, 1, 8, 3))
colnames(exponent) <- c("exponent")
rownames(exponent) <- c("K", "M", "", "B", "m", "+", "0", "5", "6", "?", "4", "2", "3", "h", "7", "H", "-", "1", "8", "k")
originalData[originalData$PROPDMGEXP=="", "PROPDMGEXP"] <- "0"
originalData[originalData$CROPDMGEXP=="", "CROPDMGEXP"] <- "0"

# Compute the actual damage values and bind the new columns to the data frame
originalData$realPropDamage <- originalData$PROPDMG * 10^exponent[originalData$PROPDMGEXP, ]
originalData$realCropDamage <- originalData$CROPDMG * 10^exponent[originalData$CROPDMGEXP, ]

# List of valid states, according to ANSI, here: https://en.wikipedia.org/wiki/List_of_U.S._state_abbreviations
validStates <- c("AL", "AK", "AZ", "AR", "CA", "CO", "CT", "DE", "DC", "FL", "GA", "HI", "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME", "MD", "MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ", "NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC", "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY", "AS", "GU", "MP", "PR", "VI", "UM", "FM", "MH", "PW")

# We need to weed out any rows with an invalid STATE. There is likely a better way to do this...
totalCropDamageByState <- aggregate(realCropDamage~STATE, data=originalData, FUN=sum)
invalidRecordsBecauseOfState <- dim(originalData[(originalData$STATE %in% setdiff(totalCropDamageByState$STATE, validStates)), ])[1]
percentageInvalidRecordsBecauseOfState <- invalidRecordsBecauseOfState/dim(originalData)[1] * 100

# Replace our current data set with one that has the bad records removed
originalData <- originalData[!(originalData$STATE %in% setdiff(totalCropDamageByState$STATE, validStates)),]
```
  
Our data also has a problem with invalid states which we cannot easily work around; we computed the number of invalid rows (`r invalidRecordsBecauseOfState` rows) and decided that due to them being a small percentage of the overall data set (`r percentageInvalidRecordsBecauseOfState`%) that it was likely valid to remove them altogether.
  
The next issue has to do with the EVTYPE field. There are many unique fields here and most of them are redundant. The strategy will be to organize these into groupings with regular expression matches with an "Other" category to capture everything that doesn't fall within a chosen grouping. To make sure that we catch as much as possible, a new column called eventType will be added to the data frame where the EVTYPE field is upper-cased and trimmed of any leading/trailing spaces.
  
Category | Expression
--------|--------------
Tornado | TORNADO or FUNNEL
Large summer storm | HURRICANE or TROPICAL STORM or TYPHOON
Tidal / Coastal | TSUNAMI or SURGE or TIDE or SURF or SEAS or MARINE or RIP CURRENT or WAVE or SWELL
Lightning | LIGHTNING or LIGHTING or THUNDER
Wind | WIND or MICROBURST
Flooding | FLOOD or WATER or RAIN or PRECIP
Winter storm / Cold | BLIZZARD or SNOW or COLD or FREEZE or CHILL or ICE or HYPOTHERMIA or SLEET or WINTER or LOW TEMP or RECORD LOW or FROST
Heat / Fire | HEAT or DROUGHT or FIRE or HIGH TEMP or RECORD HIGH or HYPERTHERMIA or HOT
Dust / Fog / Smoke | DUST or FOG or SMOKE
Hail | HAIL
Avalanche / Mudslide | AVALANCHE or (?:LAND or MUD)SLIDE
Other | Everything else

```{r Data Cleaning Phase 2}
# This takes a minute to run - it has a lot of work to do (regexes are not cheap and this is running over 800K of them).
originalData$eventType <- toupper(mapply(FUN=gsub, originalData$EVTYPE, MoreArgs=list(pattern="^\\s+(.*?)\\s*$", replacement="\\1", perl=TRUE), USE.NAMES=FALSE))

# This is also not going to be cheap, because it is going to run even more regexes.
classifyEventType <- function(s) {
	result <- "Other"
	if(grepl("TORNADO|FUNNEL", s))
	{
		result <- "Tornado"
	}
	else if (grepl("HURRICANE|TROPICAL STORM|TYPHOON", s))
	{
		result <- "Hurricane"
	}
	else if (grepl("TSUNAMI|SURGE|TIDE|SURF|SEAS|MARINE|RIP CURRENT|WAVE|SWELL", s))
	{
		result <- "Tidal / Coastal"
	}
	else if (grepl("LIGHTNING|LIGHTING|THUNDER", s))
	{
		result <- "Lightning"
	}
	else if (grepl("WIND|MICROBURST", s))
	{
		result <- "Wind / Microburst"
	}
	else if (grepl("FLOOD|WATER|RAIN|PRECIP", s))
	{
		result <- "Flooding / Water"
	}
	else if (grepl("BLIZZARD|SNOW|COLD|FREEZE|CHILL|ICE|HYPOTHERMIA|SLEET|WINTER|LOW TEMP|RECORD LOW|FROST", s))
	{
		result <- "Winter Storm / Cold"
	}
	else if (grepl("HEAT|DROUGHT|FIRE|HIGH TEMP|RECORD HIGH|HYPERTHERMIA|HOT", s))
	{
		result <- "Heat / Fire"
	}
	else if (grepl("DUST|FOG|SMOKE", s))
	{
		result <- "Dust / Fog / Smoke"
	}
	else if (grepl("HAIL", s))
	{
		result <- "Hail"
	}
	else if (grepl("AVALANCHE|(?:LAND|MUD)SLIDE", s))
	{
		result <- "Avalanche / Mudslide"
	}
	result
}
originalData$eventClassification <- mapply(FUN=classifyEventType, originalData$eventType, USE.NAMES=FALSE)


fatalityCount <- aggregate(FATALITIES~eventClassification, data=originalData, FUN=sum)
fatalityCount <- fatalityCount[order(fatalityCount$FATALITIES, decreasing=T), ]
injuryCount <- aggregate(INJURIES~eventClassification, data=originalData, FUN=sum)
injuryCount <- injuryCount[order(injuryCount$INJURIES, decreasing=T), ]

propertyDamageAmount <- aggregate(realPropDamage~eventClassification, data=originalData, FUN=sum)
propertyDamageAmount <- propertyDamageAmount[order(propertyDamageAmount$realPropDamage, decreasing=T), ]
cropDamageAmount <- aggregate(realCropDamage~eventClassification, data=originalData, FUN=sum)
cropDamageAmount <- cropDamageAmount[order(cropDamageAmount$realCropDamage, decreasing=T), ]


bp <- barplot(fatalityCount$FATALITIES)
text(cex=1, x=bp-.25, y=-1.25, fatalityCount$eventClassification, xpd=TRUE, srt=45, pos=2)
bp <- barplot(injuryCount$INJURIES)
text(cex=1, x=bp-.25, y=-1.25, injuryCount$eventClassification, xpd=TRUE, srt=45, pos=2)

bp <- barplot(propertyDamageAmount$realPropDamage)
text(cex=1, x=bp-.25, y=-1.25, propertyDamageAmount$eventClassification, xpd=TRUE, srt=45, pos=2)
bp <- barplot(cropDamageAmount$realCropDamage)
text(cex=1, x=bp-.25, y=-1.25, cropDamageAmount$eventClassification, xpd=TRUE, srt=45, pos=2)
```

## Results

* Top 10 list of fatalities/injuries by state
* Top 10 list of damage costs by state
* Top overall
Talk about bang-for-the-buck as to [what type of event and where] that money should be invested



    The analysis document must have at least one figure containing a plot.

    Your analyis must have no more than three figures. Figures may have multiple plots in them (i.e. panel plots), but there cannot be more than three figures total.

    You must show all your code for the work in your analysis document. This may make the document a bit verbose, but that is okay. In general, you should ensure that echo = TRUE for every code chunk (this is the default setting in knitr).
  
## About the Data
Filenames, MD5, Sys,info(), sessionInfo()
Ref states abbreviations here: https://en.wikipedia.org/wiki/List_of_U.S._state_abbreviations

  
### What is "Other"?
This is just a quickie function to list all of the classifications that fall into the "Other" category. There are many of them, but most are summary data and mis-spellings of terms that would have been caught by the classification algorithm if they were correct. Some are rare enough or non-specific enough to deserve the "Other" classification.
  
```{r whatIsOther}
whatIsOther <- function(eventTypeData) {
other <- as.data.frame(unique(eventTypeData), stringsAsFactors=FALSE)
colnames(other) <- c("type")
other$classification <- mapply(FUN=classifyEventType, unique(eventTypeData), USE.NAMES=FALSE)
other <- other[other$classification == "Other", "type"]
other
}
whatIsOther(originalData$eventType)
```
  
  