---
title: "NOAA Weather Data Analysis 1950-2011"
author: "SDMitchell"
date: "August 15, 2015"
output:
  html_document:
    keep_md: true
---

## Synopsis
This report will attempt to analyze event data in the NOAA Storm Data database. This database contains data for the United States from the years 1950 through 2011 inclusive; the analysis will consist of attempting to answer the following two questions:
  
1. Across the United States, which types of events (as indicated in the EVTYPE variable) are most harmful with respect to population health?
  
2. Across the United States, which types of events have the greatest economic consequences?
  
*** 
Points of note
  
* The code for fetching, extracting and caching relevant portions of the data is all included as part of the analysis;
* It is important to note that all dollar amounts are in US dollars and are *NOT* adjusted for inflation.
  
*** 
  
## Data Processing
The following libraries are needed for this analysis and it is assumed that they are already installed (via install.packages) if any reproduction is being attempted:
```{r Libraries, warning=FALSE, error=FALSE, message=FALSE}
library(ggplot2)
library(tools)
library(lubridate)
library(reshape2)
library(plyr)
library(maps)
library(mapproj)

options(digits=2, scipen=1)
```
  
These variables are for our target data on the internet and our starting filenames.
  
```{r Variables}
cacheFilename <- "repdata_data_StormData.rds"
rawDataFilename <- "repdata_data_StormData.csv.bz2"
targetURL <- "https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2"
```
  
We'll start the data cleaning process by abbreviating the data set to one that continas only the features of interest. Once we have this set we store it on disk as a serialized R object for future use (and much shorter startup times). We store this data set in the variable "originalData".
  
```{r Data Acquisition}
acquireData <- function() {
	# Try to download the given file if our input file does not already exist
	if(!file.exists(cacheFilename))
	{
		# If the input data file doesn't exist yet, we need to download it
		if(!file.exists(rawDataFilename))
		{
			download.file(url=targetURL, destfile=rawDataFilename, method="auto", mode="wb")
		}
	
		if(file.exists(rawDataFilename))
		{
			# Record the downloaded file's MD5 sum; this file's creation date can serve as the downloaded date
			write(md5sum(rawDataFilename), paste(rawDataFilename, ".MD5", sep=""))
		}
		else
		{
			stop("There was a problem attempting to download the file from the given URL")
		}
		# We either have the raw data file or it was already on disk. So here we are going to create an abbreviate data set
		# with just the features we want. This allows us to get rid of the noisy (and memory-heavy) REMARKS column. Plus
		# the time columns are an error-filled mess that were obviously done by manual entry and are nigh unusable.
		dataRaw <- read.csv(rawDataFilename, header=TRUE, sep=",", stringsAsFactors=FALSE, na.strings="NA")
		dataSelection <- dataRaw[, c("BGN_DATE", "STATE", "EVTYPE", "F", "FATALITIES", "INJURIES", "PROPDMG", "PROPDMGEXP",  "CROPDMG", "CROPDMGEXP", "WFO")]
		saveRDS(dataSelection, cacheFilename)
	}
	else
	{
		dataSelection <- readRDS(cacheFilename)
	}
	
	dataSelection
}
originalData <- acquireData()
```
  
In the interest of starting with a tidy data set, we can compact the property damage and crop damage columns by simply multiplying them by their proper exponent column. There are a lot of problems in the exponents column with many entries not matching the documented accepted values, so some assumptions must be made.
  
Invalid Exponent | Assumed Value
-------- | --------
- | 1000
+ | 1000000
? | 0
h/H | 100
Numeric | equivalent number
empty string | 0

```{r Data Cleaning Phase 1}
# Create a mapping of data set value to exponent that we are going to assume it represents
exponent <- as.data.frame(c(3, 6, 0, 9, 6, 6, 0, 5, 6, 0, 4, 2, 3, 2, 7, 2, 3, 1, 8, 3))
colnames(exponent) <- c("exponent")
rownames(exponent) <- c("K", "M", "", "B", "m", "+", "0", "5", "6", "?", "4", "2", "3", "h", "7", "H", "-", "1", "8", "k")
originalData[originalData$PROPDMGEXP=="", "PROPDMGEXP"] <- "0"
originalData[originalData$CROPDMGEXP=="", "CROPDMGEXP"] <- "0"

# Compute the actual damage values and bind the new columns to the data frame
originalData$realPropDamage <- originalData$PROPDMG * 10^exponent[originalData$PROPDMGEXP, ]
originalData$realCropDamage <- originalData$CROPDMG * 10^exponent[originalData$CROPDMGEXP, ]

# List of valid states, according to ANSI, here: https://en.wikipedia.org/wiki/List_of_U.S._state_abbreviations
validStates <- as.data.frame(c("AL", "AK", "AZ", "AR", "CA", "CO", "CT", "DE", "DC", "FL", "GA", "HI", "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME", "MD", "MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ", "NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC", "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY", "AS", "GU", "MP", "PR", "VI", "UM", "FM", "MH", "PW"))
colnames(validStates) <- c("abbrev")
validStates$fullNames <- tolower(c("Alabama","Alaska","Arizona","Arkansas","California","Colorado","Connecticut","Delaware","District of Columbia","Florida","Georgia","Hawaii","Idaho","Illinois","Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland","Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada","New Hampshire","New Jersey","New Mexico","New York","North Carolina","North Dakota","Ohio","Oklahoma","Oregon","Pennsylvania","Rhode Island","South Carolina","South Dakota","Tennessee","Texas","Utah","Vermont","Virginia","Washington","West Virginia","Wisconsin","Wyoming","American Samoa","Guam","Northern Mariana Islands","Puerto Rico","Virgin Islands","U.S. Minor Outlying Islands","Federated States of Micronesia","Marshall Islands","Palau"))

# We need to weed out any rows with an invalid STATE. There is likely a better way to do this...
totalCropDamageByState <- aggregate(realCropDamage~STATE, data=originalData, FUN=sum)
invalidRecordsBecauseOfState <- dim(originalData[(originalData$STATE %in% setdiff(totalCropDamageByState$STATE, validStates$abbrev)), ])[1]
percentageInvalidRecordsBecauseOfState <- invalidRecordsBecauseOfState/dim(originalData)[1] * 100

# Replace our current data set with one that has the bad records removed
originalData <- originalData[!(originalData$STATE %in% setdiff(totalCropDamageByState$STATE, validStates$abbrev)),]
```
  
Our data also has a problem with invalid states which we cannot easily work around; we computed the number of invalid rows (`r invalidRecordsBecauseOfState` rows) and decided that due to them being a small percentage of the overall data set (`r percentageInvalidRecordsBecauseOfState`%) that it was likely valid to remove them altogether.
  
The next issue has to do with the EVTYPE field. There are many unique fields here and most of them are redundant. The strategy will be to organize these into groupings with regular expression matches with an "Other" category to capture everything that doesn't fall within a chosen grouping. To make sure that we catch as much as possible, a new column called eventType will be added to the data frame where the EVTYPE field is upper-cased and trimmed of any leading/trailing spaces. Please note that due to limitations in R Markdown processing, the regular expressions had to be slightly modified in the table to be compliant. Simply replace the phrase " or " with the regex term "|" to get the valid expression (see the code for the real expressions).
  
Category | Expression
--------|--------------
Tornado | TORNADO or FUNNEL
Large summer storm | HURRICANE or TROPICAL STORM or TYPHOON
Tidal / Coastal | TSUNAMI or SURGE or TIDE or SURF or SEAS or MARINE or RIP CURRENT or WAVE or SWELL
Lightning | LIGHTNING or LIGHTING or THUNDER
Wind | WIND or MICROBURST
Flooding | FLOOD or WATER or RAIN or PRECIP
Winter storm / Cold | BLIZZARD or SNOW or COLD or FREEZE or CHILL or ICE or HYPOTHERMIA or SLEET or WINTER or LOW TEMP or RECORD LOW or FROST
Heat / Fire | HEAT or DROUGHT or FIRE or HIGH TEMP or RECORD HIGH or HYPERTHERMIA or HOT
Dust / Fog / Smoke | DUST or FOG or SMOKE
Hail | HAIL
Avalanche / Mudslide | AVALANCHE or (?:LAND or MUD)SLIDE
Other | Everything else

```{r Data Cleaning Phase 2}
# This takes a minute to run - it has a lot of work to do (regexes are not cheap and this is running over 800K of them).
originalData$eventType <- toupper(mapply(FUN=gsub, originalData$EVTYPE, MoreArgs=list(pattern="^\\s+(.*?)\\s*$", replacement="\\1", perl=TRUE), USE.NAMES=FALSE))

# This is also not going to be cheap, because it is going to run even more regexes. This is the classification function which is the heart of the analysis; there is an appendix at the end of the report if there is any curiousity as to what EVTYPEs fall into the "Other" category.
classifyEventType <- function(s) {
	result <- "Other"
	if(grepl("TORNADO|FUNNEL", s))
	{
		result <- "Tornado"
	}
	else if (grepl("HURRICANE|TROPICAL STORM|TYPHOON", s))
	{
		result <- "Hurricane"
	}
	else if (grepl("TSUNAMI|SURGE|TIDE|SURF|SEAS|MARINE|RIP CURRENT|WAVE|SWELL", s))
	{
		result <- "Tidal / Coastal"
	}
	else if (grepl("LIGHTNING|LIGHTING|THUNDER", s))
	{
		result <- "Lightning"
	}
	else if (grepl("WIND|MICROBURST", s))
	{
		result <- "Wind / Microburst"
	}
	else if (grepl("FLOOD|WATER|RAIN|PRECIP", s))
	{
		result <- "Flooding / Water"
	}
	else if (grepl("BLIZZARD|SNOW|COLD|FREEZE|CHILL|ICE|HYPOTHERMIA|SLEET|WINTER|LOW TEMP|RECORD LOW|FROST", s))
	{
		result <- "Winter Storm / Cold"
	}
	else if (grepl("HEAT|DROUGHT|FIRE|HIGH TEMP|RECORD HIGH|HYPERTHERMIA|HOT", s))
	{
		result <- "Heat / Fire"
	}
	else if (grepl("DUST|FOG|SMOKE", s))
	{
		result <- "Dust / Fog / Smoke"
	}
	else if (grepl("HAIL", s))
	{
		result <- "Hail"
	}
	else if (grepl("AVALANCHE|(?:LAND|MUD)SLIDE", s))
	{
		result <- "Avalanche / Mudslide"
	}
	result
}
originalData$eventClassification <- mapply(FUN=classifyEventType, originalData$eventType, USE.NAMES=FALSE)

# This is used for the population plot
fatalityCount <- aggregate(FATALITIES~eventClassification, data=originalData, FUN=sum)
fatalityCount <- fatalityCount[order(fatalityCount$FATALITIES, decreasing=T), ]
injuryCount <- aggregate(INJURIES~eventClassification, data=originalData, FUN=sum)
injuryCount <- injuryCount[order(injuryCount$INJURIES, decreasing=T), ]
victimCount <- merge(fatalityCount, injuryCount, by= "eventClassification")
populationHealthData <- melt(victimCount, id.vars=c("eventClassification"))
victimCount$total <- victimCount$FATALITIES + victimCount$INJURIES

# This is used for the economic impact plot
propertyDamageAmount <- aggregate(realPropDamage~eventClassification, data=originalData, FUN=sum)
propertyDamageAmount <- propertyDamageAmount[order(propertyDamageAmount$realPropDamage, decreasing=T), ]
cropDamageAmount <- aggregate(realCropDamage~eventClassification, data=originalData, FUN=sum)
cropDamageAmount <- cropDamageAmount[order(cropDamageAmount$realCropDamage, decreasing=T), ]
totalDamageAmount <- merge(propertyDamageAmount, cropDamageAmount, by= "eventClassification")
economicHealthData <-melt(totalDamageAmount, id.vars=c("eventClassification"))
totalDamageAmount$total <- totalDamageAmount$realCropDamage + totalDamageAmount$realPropDamage

# These are for our post-analysis choropleths

# Water damage
waterDamageByState <- by(originalData, originalData$STATE, function(x) sum(x[x$eventClassification=="Flooding / Water", "realCropDamage"] + x[x$eventClassification=="Flooding / Water", "realPropDamage"]))

# We need a data frame to really do anything of use with "by" results
dfWaterDamageByState <- as.data.frame(names(waterDamageByState))
colnames(dfWaterDamageByState) <- c("state")
dfWaterDamageByState$cost <- as.vector(waterDamageByState[names(waterDamageByState)])
dfWaterDamageByState <- merge(dfWaterDamageByState, validStates, by.x="state", by.y="abbrev")
colnames(dfWaterDamageByState) <- c("state", "cost", "stateName")

# Tornado victims
tornadoVictimsByState <- by(originalData, originalData$STATE, function(x) sum(x[x$eventClassification=="Tornado", "FATALITIES"] + x[x$eventClassification=="Tornado", "INJURIES"]))

# We need a data frame to really do anything of use with "by" results
dfTornadoVictimsByState <- as.data.frame(names(tornadoVictimsByState))
colnames(dfTornadoVictimsByState) <- c("state")
dfTornadoVictimsByState$victims <- as.vector(tornadoVictimsByState[names(tornadoVictimsByState)])
dfTornadoVictimsByState <- merge(dfTornadoVictimsByState, validStates, by.x="state", by.y="abbrev")
colnames(dfTornadoVictimsByState) <- c("state", "victims", "stateName")
```
  
*** 
  
## Results
To answer the questions posed in the Synopsis section, we have constructed three plots. 
  
**1. Across the United States, which types of events (as indicated in the EVTYPE variable) are most harmful with respect to population health?**
  
After classifying the various types of events and performing some aggregations based thereon, it was very apparent that **tornado activity** was overwhelmingly responsible for causing the most harm to humans - to the point that a bar chart is uninteresting and not presented here (although the code to reproduce it is relevant):
  
```{r Victim Count Graph, fig.width=12}
# Stacked bar plot fatality/injury
g <- ggplot(populationHealthData, aes(eventClassification, value/1000, order=desc(variable)))
g <- g + geom_bar(stat="identity", aes(fill=variable), colour="black") +
	labs(x = "Event Type", y = "Victims (Thousands)", fill="Incident Type", title="US Total Victims Per Weather Phenomenon 1950 - 2011") +
	scale_fill_discrete(labels=c("Fatalities", "Injuries")) +
	theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1), 
		  legend.background = element_rect(fill = "#FFFAF0", colour = "#000000"), 
		  legend.position = c(1,1), legend.justification = c( 1,1),
		  panel.background = element_rect(fill = "#FFFAF0", colour = "#000000"), 
		  panel.background = element_rect(fill = "#FFFAF0", colour = "#000000"), 
		  plot.background = element_rect(fill = "#FFFAF0", colour = "#000000"), 
		  panel.grid.major = element_line(colour="#FFDEAD"))
#g
```
  
```{r Victim Count Table}
victimCount[order(victimCount$total, decreasing=T), ]
```
  
The per-state breakdown for tornado harm is interesting but unsurprising, as tornado activity in the United States happens in a rather well-defined area. A choropleth of the victim count shows Texas as the most victimized by injury/fatality:
  
```{r Tornado Victim Count, fig.width=12, fig.align='center'}
states_map <- map_data("state") 
pal <- colorRampPalette(c("#00FF00", "#FF0000"))(5)

g <- ggplot(dfTornadoVictimsByState, aes(map_id=stateName, fill=victims))
g <- g + geom_map(map=states_map, colour ="black") + 
	scale_fill_gradient2(low=pal[1], mid=pal[3], high=pal[5], midpoint=median(dfTornadoVictimsByState$victims)) +
	labs(x = "Longitude (deg)", y = "Latitude (Deg)", fill="Victims", title="US Per-State Victims Due To Tornado-Related Weather Affects 1950-2011") +
	expand_limits(x = states_map$long, y = states_map$lat) +
	coord_map("polyconic") +
	theme(legend.background = element_rect(fill = "#FFFAF0", colour = "#000000"), 
		  panel.background = element_rect(fill = "#FFFAF0", colour = "#000000"), 
		  panel.background = element_rect(fill = "#FFFAF0", colour = "#000000"), 
		  plot.background = element_rect(fill = "#FFFAF0", colour = "#000000"), 
		  panel.grid.major = element_line(colour="#FFDEAD"))
g
```
  
```{r Top-10 Victim Count}
head(dfTornadoVictimsByState[order(dfTornadoVictimsByState$victims, decreasing=T), c("stateName", "state", "victims")], 10)
```
  
**2. Across the United States, which types of events have the greatest economic consequences?**
  
Economic consequences were a bit more evenly spread out than the population harm results, although **flooding and water-related damages** seemed to clearly be the worst for property damage and **heat-related weather conditions** were very bad in terms of crop damage:
  
```{r Economic Impact Graph, fig.width=12, fig.align='center'}
# Stacked bar plot prop/crop damage
g <- ggplot(economicHealthData, aes(eventClassification, value/1000000000))
g <- g + geom_bar(stat="identity", aes(fill=variable), colour="black") +
	guides(fill = guide_legend(reverse = TRUE)) +
	labs(x = "Event Type", y = "Cost (USD Billions, non-adjusted)", fill="Damage Type", title="US Total Cost Per Weather Phenomenon 1950 - 2011") +
	scale_fill_discrete(labels=c("Property", "Crop")) +
	theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1), 
		  legend.background = element_rect(fill = "#FFFAF0", colour = "#000000"), 
		  legend.position = c(1,1), legend.justification = c( 1,1),
		  panel.background = element_rect(fill = "#FFFAF0", colour = "#000000"), 
		  panel.background = element_rect(fill = "#FFFAF0", colour = "#000000"), 
		  plot.background = element_rect(fill = "#FFFAF0", colour = "#000000"), 
		  panel.grid.major = element_line(colour="#FFDEAD"))
g
```
  
```{r Economic Impact Table}
totalDamageAmount[order(totalDamageAmount$total, decreasing=T), ]
```
  
A per-state breakdown of the economic impact of these weather events seems to place California overwhelmingly in the lead in terms of damage. This is likely due to being a coastal area with many expensive properties. California is also responsible for a large amount of food production for the entire United States, leading to an unbalanced amount of damage per square mile.

```{r Economic Impact choropleth, fig.width=12, fig.align='center'}
states_map <- map_data("state") 
pal <- colorRampPalette(c("#00FF00", "#FF0000"))(5)

g <- ggplot(dfWaterDamageByState, aes(map_id=stateName, fill=log(cost)))
g <- g + geom_map(map=states_map, colour ="black") + 
	scale_fill_gradient2(low=pal[1], mid=pal[3], high=pal[5], midpoint=median(log(dfWaterDamageByState$cost))) +
	labs(x = "Longitude (deg)", y = "Latitude (Deg)", fill="Log-Cost", title="US Per-State Cost Due To Water-Related Weather Affects 1950-2011") +
	expand_limits(x = states_map$long, y = states_map$lat) +
	coord_map("polyconic") +
	theme(legend.background = element_rect(fill = "#FFFAF0", colour = "#000000"), 
		  panel.background = element_rect(fill = "#FFFAF0", colour = "#000000"), 
		  panel.background = element_rect(fill = "#FFFAF0", colour = "#000000"), 
		  plot.background = element_rect(fill = "#FFFAF0", colour = "#000000"), 
		  panel.grid.major = element_line(colour="#FFDEAD"))
g
```
  
```{r Top-10 Economic Impact}
head(dfWaterDamageByState[order(dfWaterDamageByState$cost, decreasing=T), c("stateName", "state", "cost")], 10)
```
  
*** 
  
## About the Analysis
  
### Metadata
File Characteristic | Value
----------------- | -------
File Name | repdata_data_StormData.csv.bz2
URL | [https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2](https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2)
File Size | 49,177,144 bytes
Compression | bz2
MD5 | df4aa61fff89427db6b7f7b1113b5553

### Environment
**System Information**
```{r}
sysinf <- Sys.info()
```
Parameter | Value
-------- | --------
Operating System | `r paste(sysinf["sysname"], sysinf["release"], sep="")`
version | `r sysinf["version"]`
machine arch | `r sysinf["machine"]`

**Session Information**
```{r}
sessionInfo()
```
  
## References
* [Wiki for United States state abbreviations](https://en.wikipedia.org/wiki/List_of_U.S._state_abbreviations)
* [R Graphics Cookbook](http://shop.oreilly.com/product/0636920023135.do) - Winston Chang
  
*** 
  
### What is "Other"?
This is just a quickie function to list all of the classifications that fall into the "Other" category. There are many of them, but most are summary data and mis-spellings of terms that would have been caught by the classification algorithm if they were correct. Some are rare enough or non-specific enough to deserve the "Other" classification.
  
```{r whatIsOther}
whatIsOther <- function(eventTypeData) {
other <- as.data.frame(unique(eventTypeData), stringsAsFactors=FALSE)
colnames(other) <- c("type")
other$classification <- mapply(FUN=classifyEventType, unique(eventTypeData), USE.NAMES=FALSE)
other <- other[other$classification == "Other", "type"]
other
}
whatIsOther(originalData$eventType)
```
  
  